{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9476110c",
   "metadata": {},
   "source": [
    "Transfer Learning using Audioset Weights and Classifier Head with 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from srcv2full.model import YAMNet\n",
    "from srcv2full.feature_extraction import WaveformToMelSpec\n",
    "import srcv2full.params as params\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load pretrained model (AudioSet weights)\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = YAMNet()\n",
    "current_state_dict = torch.load(\"checkpoints/yamnet_audioset_converted.pth\", map_location=device)\n",
    "\n",
    "# Fix state_dict keys\n",
    "new_state_dict = {}\n",
    "for k, v in current_state_dict.items():\n",
    "    if k.startswith(\"layer.\"):\n",
    "        parts = k.split(\".\")\n",
    "        layer_idx = int(parts[1]) + 1\n",
    "        new_key = f\"layer_{layer_idx}.\" + \".\".join(parts[2:])\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load weights but ignore classifier mismatch\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Replace classifier for ESC50Artifact (7 classes)\n",
    "model.classifier = nn.Linear(1024, 7, bias=True)\n",
    "\n",
    "# Freeze all layers except classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load & preprocess an audio file\n",
    "# -----------------------------\n",
    "audio_path = \"ESC50Artifact/audio/1-7973-A-7_hiss.wav\"  # replace with your audio file\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# Resample if needed\n",
    "if sr != params.SAMPLE_RATE:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, params.SAMPLE_RATE)\n",
    "\n",
    "# Ensure mono\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "# Convert waveform to Mel Spectrogram chunks\n",
    "waveform_to_mel = WaveformToMelSpec(device=device)\n",
    "x_chunks, mel_spectrogram = waveform_to_mel(waveform, params.SAMPLE_RATE)\n",
    "x_chunks = x_chunks.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Run forward pass (inference)\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    logits = model(x_chunks)  # [num_chunks, 7]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Average predictions across chunks\n",
    "avg_probs = probs.mean(dim=0)\n",
    "pred_class = torch.argmax(avg_probs).item()\n",
    "confidence = avg_probs[pred_class].item()\n",
    "\n",
    "print(f\"Predicted class: {pred_class}, confidence: {confidence:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Visualization\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(waveform.squeeze().cpu().numpy())\n",
    "plt.title(f\"Waveform\\nPredicted class: {pred_class}, Confidence: {confidence:.3f}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(mel_spectrogram.squeeze(), aspect='auto', origin='lower', cmap='seismic',\n",
    "           extent=[0, mel_spectrogram.shape[1], 0, params.SAMPLE_RATE])\n",
    "plt.title(\"Mel Spectrogram (Reds)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(range(7), avg_probs.cpu().numpy())\n",
    "plt.title(\"Prediction Probabilities\")\n",
    "plt.xlabel(\"Class ID (0–6)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d31df",
   "metadata": {},
   "source": [
    "Finetuning first on ESC50Artifact with only classifier unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:YAMNetTraining:Using CUDA\n",
      "INFO:YAMNetTraining:NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "INFO:YAMNetTraining:Started training\n",
      "Epoch 1/10 [Train]:   0%|          | 0/2800 [00:00<?, ?it/s]/home/iam4tt4/miniconda3/envs/GML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|██████████| 2800/2800 [01:47<00:00, 26.01it/s, loss=2.37]\n",
      "INFO:YAMNetTraining:Epoch 1/10, Train Loss: 1.9768, Train Acc: 0.0333\n",
      "Epoch 1/10 [Val]: 100%|██████████| 700/700 [00:22<00:00, 31.04it/s, loss=2.32]  \n",
      "INFO:YAMNetTraining:Epoch 1/10, Val Loss: 2.9743, Val Acc: 0.0108\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0108\n",
      "Epoch 2/10 [Train]: 100%|██████████| 2800/2800 [01:48<00:00, 25.72it/s, loss=1.81]\n",
      "INFO:YAMNetTraining:Epoch 2/10, Train Loss: 1.9760, Train Acc: 0.0342\n",
      "Epoch 2/10 [Val]: 100%|██████████| 700/700 [00:23<00:00, 29.72it/s, loss=2.32] \n",
      "INFO:YAMNetTraining:Epoch 2/10, Val Loss: 2.1432, Val Acc: 0.0081\n",
      "Epoch 3/10 [Train]: 100%|██████████| 2800/2800 [01:52<00:00, 24.97it/s, loss=1.69]\n",
      "INFO:YAMNetTraining:Epoch 3/10, Train Loss: 1.9732, Train Acc: 0.0361\n",
      "Epoch 3/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 32.22it/s, loss=1.7]  \n",
      "INFO:YAMNetTraining:Epoch 3/10, Val Loss: 2.5378, Val Acc: 0.0095\n",
      "Epoch 4/10 [Train]: 100%|██████████| 2800/2800 [01:47<00:00, 26.15it/s, loss=1.79]\n",
      "INFO:YAMNetTraining:Epoch 4/10, Train Loss: 1.9772, Train Acc: 0.0343\n",
      "Epoch 4/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.17it/s, loss=2.28]   \n",
      "INFO:YAMNetTraining:Epoch 4/10, Val Loss: 3.0162, Val Acc: 0.0129\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0129\n",
      "Epoch 5/10 [Train]: 100%|██████████| 2800/2800 [01:40<00:00, 27.84it/s, loss=2.05]\n",
      "INFO:YAMNetTraining:Epoch 5/10, Train Loss: 1.9764, Train Acc: 0.0354\n",
      "Epoch 5/10 [Val]: 100%|██████████| 700/700 [00:24<00:00, 28.98it/s, loss=1.57]  \n",
      "INFO:YAMNetTraining:Epoch 5/10, Val Loss: 4.3358, Val Acc: 0.0083\n",
      "Epoch 6/10 [Train]: 100%|██████████| 2800/2800 [01:48<00:00, 25.89it/s, loss=1.83]\n",
      "INFO:YAMNetTraining:Epoch 6/10, Train Loss: 1.9718, Train Acc: 0.0359\n",
      "Epoch 6/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 31.90it/s, loss=2.42] \n",
      "INFO:YAMNetTraining:Epoch 6/10, Val Loss: 3.4744, Val Acc: 0.0191\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0191\n",
      "Epoch 7/10 [Train]: 100%|██████████| 2800/2800 [01:41<00:00, 27.69it/s, loss=2.2] \n",
      "INFO:YAMNetTraining:Epoch 7/10, Train Loss: 1.9753, Train Acc: 0.0346\n",
      "Epoch 7/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 33.85it/s, loss=2.29]    \n",
      "INFO:YAMNetTraining:Epoch 7/10, Val Loss: 5.8202, Val Acc: 0.0094\n",
      "Epoch 8/10 [Train]: 100%|██████████| 2800/2800 [01:39<00:00, 28.08it/s, loss=1.62]\n",
      "INFO:YAMNetTraining:Epoch 8/10, Train Loss: 1.9762, Train Acc: 0.0369\n",
      "Epoch 8/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 32.64it/s, loss=2.44] \n",
      "INFO:YAMNetTraining:Epoch 8/10, Val Loss: 3.0463, Val Acc: 0.0112\n",
      "Epoch 9/10 [Train]: 100%|██████████| 2800/2800 [01:40<00:00, 27.86it/s, loss=1.71]\n",
      "INFO:YAMNetTraining:Epoch 9/10, Train Loss: 1.9706, Train Acc: 0.0377\n",
      "Epoch 9/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.00it/s, loss=1.96] \n",
      "INFO:YAMNetTraining:Epoch 9/10, Val Loss: 2.7022, Val Acc: 0.0122\n",
      "Epoch 10/10 [Train]: 100%|██████████| 2800/2800 [01:38<00:00, 28.49it/s, loss=1.74]\n",
      "INFO:YAMNetTraining:Epoch 10/10, Train Loss: 1.9772, Train Acc: 0.0361\n",
      "Epoch 10/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.25it/s, loss=1.65] \n",
      "INFO:YAMNetTraining:Epoch 10/10, Val Loss: 2.0801, Val Acc: 0.0079\n",
      "INFO:YAMNetTraining:Training completed. Runtime: 0:21:01.845145\n",
      "INFO:YAMNetTraining:Best validation accuracy: 0.0191\n",
      "INFO:YAMNetTraining:Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import srcv2full.params as params\n",
    "from srcv2full.data import ESC50ArtifactData\n",
    "from srcv2full.model import YAMNet\n",
    "from srcv2full.engine import YAMNetEngine\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Logger setup\n",
    "# -----------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"YAMNetTraining\")\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load pretrained model\n",
    "# -----------------------------\n",
    "model = YAMNet()\n",
    "checkpoint = torch.load(\"checkpoints/yamnet_audioset_converted.pth\", map_location=device)\n",
    "\n",
    "# Fix layer names if needed\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint.items():\n",
    "    if k.startswith(\"layer.\"):\n",
    "        parts = k.split(\".\")\n",
    "        layer_idx = int(parts[1]) + 1\n",
    "        new_key = f\"layer_{layer_idx}.\" + \".\".join(parts[2:])\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Replace classifier for 7 classes\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 7)\n",
    "\n",
    "# Freeze backbone\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset and DataLoader\n",
    "# -----------------------------\n",
    "data_dir = \"ESC50Artifact/\"\n",
    "full_dataset = ESC50ArtifactData(data_dir)\n",
    "\n",
    "# Split 80/20 train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# -----------------------------\n",
    "# Engine\n",
    "# -----------------------------\n",
    "tt_chunk_size = params.CHUNK_SIZE\n",
    "engine = YAMNetEngine(model=model, tt_chunk_size=tt_chunk_size, logger=logger)\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "checkpoint_path = \"checkpoints/yamnet_finetune_esc50artifact.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "engine.train_yamnet(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_labels=7,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "logger.info(\"v1 Fine-tuning complete! -1 unfrozen\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565291c",
   "metadata": {},
   "source": [
    "```python\n",
    "1. Backbone is still frozen\n",
    "\n",
    "In your current setup, if you froze all the pre-trained layers, the classifier alone might not have enough capacity to learn meaningful features from ESC50Artifact.\n",
    "\n",
    "Especially since AudioSet classes (521) are very different from your 7 artifact classes.\n",
    "\n",
    "Freezing everything except the last layer works well only if your dataset is very similar to the pre-trained one.\n",
    "\n",
    "✅ Solution: Unfreeze at least the last few convolutional/separable layers of YAMNet, not just the classifier.\n",
    "\n",
    "2. Learning rate too low\n",
    "\n",
    "For the classifier alone, 0.0001 may be okay.\n",
    "\n",
    "But if you unfreeze some backbone layers, you’ll likely need a slightly lower LR for the backbone and higher for the classifier.\n",
    "\n",
    "3. Loss & chunk averaging\n",
    "\n",
    "Currently, your forward averages first chunk only for loss.\n",
    "\n",
    "You should average all chunks’ predictions per audio sample before computing the loss. Otherwise, the classifier gets extremely noisy gradients.\n",
    "\n",
    "4. Data normalization\n",
    "\n",
    "If your Mel spectrogram preprocessing is different from what AudioSet training used, the backbone may not recognize features well.\n",
    "\n",
    "You may need same normalization/scaling as pre-trained model.\n",
    "\n",
    "5. Tiny dataset\n",
    "\n",
    "ESC50Artifact is small; the model may overfit or barely learn if backbone is frozen.\n",
    "\n",
    "Consider data augmentation (e.g., noise, pitch shift, time stretch) to help fine-tuning.\n",
    "\n",
    "✅ Recommended Steps\n",
    "\n",
    "Unfreeze last few YAMNet layers, e.g., layer_12, layer_13, and the classifier:\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer_12\" in name or \"layer_13\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "Use separate learning rates for backbone vs classifier:\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": [p for n, p in model.named_parameters() if \"layer_12\" in n or \"layer_13\" in n], \"lr\": 1e-4},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": 5e-4}\n",
    "])\n",
    "\n",
    "\n",
    "Average predictions over all chunks per sample for training loss.\n",
    "\n",
    "Optional: Add data augmentation for each audio batch.\n",
    "\n",
    "Not strictly — your ESC50ArtifactData class is fine structurally — it correctly:\n",
    "\n",
    "Loads audio\n",
    "\n",
    "Converts multi-channel to mono\n",
    "\n",
    "Returns integer labels for classification\n",
    "\n",
    "…but there are a few optional improvements that can help fine-tuning:\n",
    "\n",
    "1. Ensure all audio is long enough\n",
    "\n",
    "Your current chunking in WaveformToMelSpec might pad very short files, which can create lots of near-zero chunks.\n",
    "\n",
    "Option: Filter out extremely short audio or pad intelligently.\n",
    "\n",
    "if waveform.shape[1] < min_length_samples:\n",
    "    waveform = torch.nn.functional.pad(waveform, (0, min_length_samples - waveform.shape[1]))\n",
    "\n",
    "2. Data augmentation\n",
    "\n",
    "For small datasets like ESC50Artifact, augmentations help the model generalize:\n",
    "\n",
    "# Example augmentations\n",
    "def augment(waveform, sr):\n",
    "    # Random noise\n",
    "    waveform += 0.005 * torch.randn_like(waveform)\n",
    "    \n",
    "    # Random pitch shift\n",
    "    n_steps = torch.randint(-2, 3, (1,)).item()\n",
    "    waveform = torchaudio.functional.pitch_shift(waveform, sr, n_steps)\n",
    "    \n",
    "    # Random time stretch\n",
    "    rate = 0.9 + 0.2 * torch.rand(1).item()  # 0.9-1.1x\n",
    "    waveform = torchaudio.functional.time_stretch(waveform, sr, rate)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "\n",
    "You can apply this in __getitem__ during training only (not validation).\n",
    "\n",
    "3. Label consistency\n",
    "\n",
    "Make sure artifact2id mapping is always deterministic: sorted(self.annotations['artifact_label'].unique()) so the same label always maps to the same index.\n",
    "\n",
    "self.artifact2id = {lbl: i for i, lbl in enumerate(sorted(self.annotations['artifact_label'].unique()))}\n",
    "\n",
    "4. Optional normalization\n",
    "\n",
    "If your pre-trained YAMNet used normalized Mel spectrograms, scale your input similarly:\n",
    "\n",
    "mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "\n",
    "✅ TL;DR\n",
    "\n",
    "No major changes needed.\n",
    "\n",
    "Recommended: data augmentation, label sorting, and padding very short audio.\n",
    "\n",
    "Your current dataset class is already compatible with your engine.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c03398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:YAMNetTraining:Using CUDA\n",
      "INFO:YAMNetTraining:NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "INFO:YAMNetTraining:Started training\n",
      "Epoch 1/10 [Train]:   0%|          | 0/2800 [00:00<?, ?it/s]/home/iam4tt4/miniconda3/envs/GML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|██████████| 2800/2800 [01:21<00:00, 34.27it/s, loss=1.82] \n",
      "INFO:YAMNetTraining:Epoch 1/10, Train Loss: 2.0976, Train Acc: 0.0349\n",
      "Epoch 1/10 [Val]: 100%|██████████| 700/700 [00:16<00:00, 43.23it/s, loss=2.49]  \n",
      "INFO:YAMNetTraining:Epoch 1/10, Val Loss: 2.6005, Val Acc: 0.0081\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0081\n",
      "Epoch 2/10 [Train]: 100%|██████████| 2800/2800 [01:19<00:00, 35.22it/s, loss=2.22] \n",
      "INFO:YAMNetTraining:Epoch 2/10, Train Loss: 2.0899, Train Acc: 0.0384\n",
      "Epoch 2/10 [Val]: 100%|██████████| 700/700 [00:15<00:00, 44.27it/s, loss=1.65] \n",
      "INFO:YAMNetTraining:Epoch 2/10, Val Loss: 2.6293, Val Acc: 0.0101\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0101\n",
      "Epoch 3/10 [Train]: 100%|██████████| 2800/2800 [01:16<00:00, 36.64it/s, loss=3.16] \n",
      "INFO:YAMNetTraining:Epoch 3/10, Train Loss: 2.0786, Train Acc: 0.0371\n",
      "Epoch 3/10 [Val]: 100%|██████████| 700/700 [00:15<00:00, 45.09it/s, loss=3.05]  \n",
      "INFO:YAMNetTraining:Epoch 3/10, Val Loss: 2.9893, Val Acc: 0.0081\n",
      "Epoch 4/10 [Train]: 100%|██████████| 2800/2800 [01:21<00:00, 34.56it/s, loss=0.899]\n",
      "INFO:YAMNetTraining:Epoch 4/10, Train Loss: 2.0491, Train Acc: 0.0391\n",
      "Epoch 4/10 [Val]: 100%|██████████| 700/700 [00:16<00:00, 42.70it/s, loss=2.67]    \n",
      "INFO:YAMNetTraining:Epoch 4/10, Val Loss: 8.0589, Val Acc: 0.0082\n",
      "Epoch 5/10 [Train]: 100%|██████████| 2800/2800 [01:21<00:00, 34.47it/s, loss=1.7]  \n",
      "INFO:YAMNetTraining:Epoch 5/10, Train Loss: 2.0409, Train Acc: 0.0421\n",
      "Epoch 5/10 [Val]: 100%|██████████| 700/700 [00:14<00:00, 48.85it/s, loss=3.67]    \n",
      "INFO:YAMNetTraining:Epoch 5/10, Val Loss: 5.4319, Val Acc: 0.0086\n",
      "Epoch 6/10 [Train]: 100%|██████████| 2800/2800 [01:14<00:00, 37.76it/s, loss=2.07] \n",
      "INFO:YAMNetTraining:Epoch 6/10, Train Loss: 2.0016, Train Acc: 0.0448\n",
      "Epoch 6/10 [Val]: 100%|██████████| 700/700 [00:14<00:00, 49.18it/s, loss=3.07]    \n",
      "INFO:YAMNetTraining:Epoch 6/10, Val Loss: 4.5781, Val Acc: 0.0063\n",
      "Epoch 7/10 [Train]: 100%|██████████| 2800/2800 [01:14<00:00, 37.45it/s, loss=1.94] \n",
      "INFO:YAMNetTraining:Epoch 7/10, Train Loss: 1.9448, Train Acc: 0.0529\n",
      "Epoch 7/10 [Val]: 100%|██████████| 700/700 [00:16<00:00, 42.02it/s, loss=3.3]    \n",
      "INFO:YAMNetTraining:Epoch 7/10, Val Loss: 5.1824, Val Acc: 0.0088\n",
      "Epoch 8/10 [Train]: 100%|██████████| 2800/2800 [01:21<00:00, 34.46it/s, loss=2.03] \n",
      "INFO:YAMNetTraining:Epoch 8/10, Train Loss: 1.8890, Train Acc: 0.0616\n",
      "Epoch 8/10 [Val]: 100%|██████████| 700/700 [00:16<00:00, 43.12it/s, loss=2.53]    \n",
      "INFO:YAMNetTraining:Epoch 8/10, Val Loss: 6.9921, Val Acc: 0.0076\n",
      "Epoch 9/10 [Train]: 100%|██████████| 2800/2800 [01:12<00:00, 38.56it/s, loss=2.02]  \n",
      "INFO:YAMNetTraining:Epoch 9/10, Train Loss: 1.7826, Train Acc: 0.0722\n",
      "Epoch 9/10 [Val]: 100%|██████████| 700/700 [00:13<00:00, 50.45it/s, loss=3.12]    \n",
      "INFO:YAMNetTraining:Epoch 9/10, Val Loss: 10.2620, Val Acc: 0.0078\n",
      "Epoch 10/10 [Train]: 100%|██████████| 2800/2800 [01:21<00:00, 34.47it/s, loss=1.84]   \n",
      "INFO:YAMNetTraining:Epoch 10/10, Train Loss: 1.6733, Train Acc: 0.0821\n",
      "Epoch 10/10 [Val]: 100%|██████████| 700/700 [00:16<00:00, 43.42it/s, loss=2.64]    \n",
      "INFO:YAMNetTraining:Epoch 10/10, Val Loss: 7.1700, Val Acc: 0.0076\n",
      "INFO:YAMNetTraining:Training completed. Runtime: 0:15:39.426065\n",
      "INFO:YAMNetTraining:Best validation accuracy: 0.0101\n",
      "INFO:YAMNetTraining:v2 Fine-tuning complete! -3 unfrozen\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import srcv2full.params as params\n",
    "from srcv2full.data import ESC50ArtifactData\n",
    "from srcv2full.model import YAMNet\n",
    "from srcv2full.engine import YAMNetEngine\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Logger setup\n",
    "# -----------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"YAMNetTraining\")\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load pretrained model\n",
    "# -----------------------------\n",
    "model = YAMNet()\n",
    "checkpoint = torch.load(\"checkpoints/yamnet_audioset_converted.pth\", map_location=device)\n",
    "\n",
    "# Fix layer names if needed\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint.items():\n",
    "    if k.startswith(\"layer.\"):\n",
    "        parts = k.split(\".\")\n",
    "        layer_idx = int(parts[1]) + 1\n",
    "        new_key = f\"layer_{layer_idx}.\" + \".\".join(parts[2:])\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Replace classifier for 7 classes\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 7)\n",
    "\n",
    "# Freeze backbone\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer_12\" in name or \"layer_13\" in name or \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset and DataLoader\n",
    "# -----------------------------\n",
    "data_dir = \"ESC50Artifact/\"\n",
    "full_dataset = ESC50ArtifactData(data_dir)\n",
    "\n",
    "# Split 80/20 train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# -----------------------------\n",
    "# Engine\n",
    "# -----------------------------\n",
    "tt_chunk_size = params.CHUNK_SIZE\n",
    "engine = YAMNetEngine(model=model, tt_chunk_size=tt_chunk_size, logger=logger)\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "checkpoint_path = \"checkpoints/yamnet_finetune_esc50artifact_frozen_-3.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "engine.train_yamnet(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_labels=7,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "logger.info(\"v2 Fine-tuning complete! -3 unfrozen\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GML",
   "language": "python",
   "name": "gml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
