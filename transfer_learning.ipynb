{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9476110c",
   "metadata": {},
   "source": [
    "Transfer Learning using Audioset Weights and Classifier Head with 7 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from srcv2full.model import YAMNet\n",
    "from srcv2full.feature_extraction import WaveformToMelSpec\n",
    "import srcv2full.params as params\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load pretrained model (AudioSet weights)\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = YAMNet()\n",
    "current_state_dict = torch.load(\"checkpoints/yamnet_audioset_converted.pth\", map_location=device)\n",
    "\n",
    "# Fix state_dict keys\n",
    "new_state_dict = {}\n",
    "for k, v in current_state_dict.items():\n",
    "    if k.startswith(\"layer.\"):\n",
    "        parts = k.split(\".\")\n",
    "        layer_idx = int(parts[1]) + 1\n",
    "        new_key = f\"layer_{layer_idx}.\" + \".\".join(parts[2:])\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "# Load weights but ignore classifier mismatch\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Replace classifier for ESC50Artifact (7 classes)\n",
    "model.classifier = nn.Linear(1024, 7, bias=True)\n",
    "\n",
    "# Freeze all layers except classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load & preprocess an audio file\n",
    "# -----------------------------\n",
    "audio_path = \"ESC50Artifact/audio/1-7973-A-7_hiss.wav\"  # replace with your audio file\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# Resample if needed\n",
    "if sr != params.SAMPLE_RATE:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, params.SAMPLE_RATE)\n",
    "\n",
    "# Ensure mono\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "# Convert waveform to Mel Spectrogram chunks\n",
    "waveform_to_mel = WaveformToMelSpec(device=device)\n",
    "x_chunks, mel_spectrogram = waveform_to_mel(waveform, params.SAMPLE_RATE)\n",
    "x_chunks = x_chunks.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Run forward pass (inference)\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    logits = model(x_chunks)  # [num_chunks, 7]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Average predictions across chunks\n",
    "avg_probs = probs.mean(dim=0)\n",
    "pred_class = torch.argmax(avg_probs).item()\n",
    "confidence = avg_probs[pred_class].item()\n",
    "\n",
    "print(f\"Predicted class: {pred_class}, confidence: {confidence:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Visualization\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(waveform.squeeze().cpu().numpy())\n",
    "plt.title(f\"Waveform\\nPredicted class: {pred_class}, Confidence: {confidence:.3f}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(mel_spectrogram.squeeze(), aspect='auto', origin='lower', cmap='seismic',\n",
    "           extent=[0, mel_spectrogram.shape[1], 0, params.SAMPLE_RATE])\n",
    "plt.title(\"Mel Spectrogram (Reds)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(range(7), avg_probs.cpu().numpy())\n",
    "plt.title(\"Prediction Probabilities\")\n",
    "plt.xlabel(\"Class ID (0–6)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83d31df",
   "metadata": {},
   "source": [
    "Finetuning first on ESC50Artifact and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2a2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:YAMNetTraining:Using CUDA\n",
      "INFO:YAMNetTraining:NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "INFO:YAMNetTraining:Started training\n",
      "Epoch 1/10 [Train]:   0%|          | 0/2800 [00:00<?, ?it/s]/home/iam4tt4/miniconda3/envs/GML/lib/python3.12/site-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|██████████| 2800/2800 [01:47<00:00, 26.01it/s, loss=2.37]\n",
      "INFO:YAMNetTraining:Epoch 1/10, Train Loss: 1.9768, Train Acc: 0.0333\n",
      "Epoch 1/10 [Val]: 100%|██████████| 700/700 [00:22<00:00, 31.04it/s, loss=2.32]  \n",
      "INFO:YAMNetTraining:Epoch 1/10, Val Loss: 2.9743, Val Acc: 0.0108\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0108\n",
      "Epoch 2/10 [Train]: 100%|██████████| 2800/2800 [01:48<00:00, 25.72it/s, loss=1.81]\n",
      "INFO:YAMNetTraining:Epoch 2/10, Train Loss: 1.9760, Train Acc: 0.0342\n",
      "Epoch 2/10 [Val]: 100%|██████████| 700/700 [00:23<00:00, 29.72it/s, loss=2.32] \n",
      "INFO:YAMNetTraining:Epoch 2/10, Val Loss: 2.1432, Val Acc: 0.0081\n",
      "Epoch 3/10 [Train]: 100%|██████████| 2800/2800 [01:52<00:00, 24.97it/s, loss=1.69]\n",
      "INFO:YAMNetTraining:Epoch 3/10, Train Loss: 1.9732, Train Acc: 0.0361\n",
      "Epoch 3/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 32.22it/s, loss=1.7]  \n",
      "INFO:YAMNetTraining:Epoch 3/10, Val Loss: 2.5378, Val Acc: 0.0095\n",
      "Epoch 4/10 [Train]: 100%|██████████| 2800/2800 [01:47<00:00, 26.15it/s, loss=1.79]\n",
      "INFO:YAMNetTraining:Epoch 4/10, Train Loss: 1.9772, Train Acc: 0.0343\n",
      "Epoch 4/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.17it/s, loss=2.28]   \n",
      "INFO:YAMNetTraining:Epoch 4/10, Val Loss: 3.0162, Val Acc: 0.0129\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0129\n",
      "Epoch 5/10 [Train]: 100%|██████████| 2800/2800 [01:40<00:00, 27.84it/s, loss=2.05]\n",
      "INFO:YAMNetTraining:Epoch 5/10, Train Loss: 1.9764, Train Acc: 0.0354\n",
      "Epoch 5/10 [Val]: 100%|██████████| 700/700 [00:24<00:00, 28.98it/s, loss=1.57]  \n",
      "INFO:YAMNetTraining:Epoch 5/10, Val Loss: 4.3358, Val Acc: 0.0083\n",
      "Epoch 6/10 [Train]: 100%|██████████| 2800/2800 [01:48<00:00, 25.89it/s, loss=1.83]\n",
      "INFO:YAMNetTraining:Epoch 6/10, Train Loss: 1.9718, Train Acc: 0.0359\n",
      "Epoch 6/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 31.90it/s, loss=2.42] \n",
      "INFO:YAMNetTraining:Epoch 6/10, Val Loss: 3.4744, Val Acc: 0.0191\n",
      "INFO:YAMNetTraining:New best model saved with val_acc: 0.0191\n",
      "Epoch 7/10 [Train]: 100%|██████████| 2800/2800 [01:41<00:00, 27.69it/s, loss=2.2] \n",
      "INFO:YAMNetTraining:Epoch 7/10, Train Loss: 1.9753, Train Acc: 0.0346\n",
      "Epoch 7/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 33.85it/s, loss=2.29]    \n",
      "INFO:YAMNetTraining:Epoch 7/10, Val Loss: 5.8202, Val Acc: 0.0094\n",
      "Epoch 8/10 [Train]: 100%|██████████| 2800/2800 [01:39<00:00, 28.08it/s, loss=1.62]\n",
      "INFO:YAMNetTraining:Epoch 8/10, Train Loss: 1.9762, Train Acc: 0.0369\n",
      "Epoch 8/10 [Val]: 100%|██████████| 700/700 [00:21<00:00, 32.64it/s, loss=2.44] \n",
      "INFO:YAMNetTraining:Epoch 8/10, Val Loss: 3.0463, Val Acc: 0.0112\n",
      "Epoch 9/10 [Train]: 100%|██████████| 2800/2800 [01:40<00:00, 27.86it/s, loss=1.71]\n",
      "INFO:YAMNetTraining:Epoch 9/10, Train Loss: 1.9706, Train Acc: 0.0377\n",
      "Epoch 9/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.00it/s, loss=1.96] \n",
      "INFO:YAMNetTraining:Epoch 9/10, Val Loss: 2.7022, Val Acc: 0.0122\n",
      "Epoch 10/10 [Train]: 100%|██████████| 2800/2800 [01:38<00:00, 28.49it/s, loss=1.74]\n",
      "INFO:YAMNetTraining:Epoch 10/10, Train Loss: 1.9772, Train Acc: 0.0361\n",
      "Epoch 10/10 [Val]: 100%|██████████| 700/700 [00:20<00:00, 34.25it/s, loss=1.65] \n",
      "INFO:YAMNetTraining:Epoch 10/10, Val Loss: 2.0801, Val Acc: 0.0079\n",
      "INFO:YAMNetTraining:Training completed. Runtime: 0:21:01.845145\n",
      "INFO:YAMNetTraining:Best validation accuracy: 0.0191\n",
      "INFO:YAMNetTraining:Fine-tuning complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import srcv2full.params as params\n",
    "from srcv2full.data import ESC50ArtifactData\n",
    "from srcv2full.model import YAMNet\n",
    "from srcv2full.engine import YAMNetEngine\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Logger setup\n",
    "# -----------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"YAMNetTraining\")\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load pretrained model\n",
    "# -----------------------------\n",
    "model = YAMNet()\n",
    "checkpoint = torch.load(\"checkpoints/yamnet_audioset_converted.pth\", map_location=device)\n",
    "\n",
    "# Fix layer names if needed\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint.items():\n",
    "    if k.startswith(\"layer.\"):\n",
    "        parts = k.split(\".\")\n",
    "        layer_idx = int(parts[1]) + 1\n",
    "        new_key = f\"layer_{layer_idx}.\" + \".\".join(parts[2:])\n",
    "        new_state_dict[new_key] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Replace classifier for 7 classes\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 7)\n",
    "\n",
    "# Freeze backbone\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset and DataLoader\n",
    "# -----------------------------\n",
    "data_dir = \"ESC50Artifact/\"\n",
    "full_dataset = ESC50ArtifactData(data_dir)\n",
    "\n",
    "# Split 80/20 train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: x)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# -----------------------------\n",
    "# Engine\n",
    "# -----------------------------\n",
    "tt_chunk_size = params.CHUNK_SIZE\n",
    "engine = YAMNetEngine(model=model, tt_chunk_size=tt_chunk_size, logger=logger)\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "checkpoint_path = \"checkpoints/yamnet_finetune_esc50artifact.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "engine.train_yamnet(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_labels=7,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "logger.info(\"Fine-tuning complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GML",
   "language": "python",
   "name": "gml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
